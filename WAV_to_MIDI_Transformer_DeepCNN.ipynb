{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**This code initializes the environment for a PyTorch-based project. It imports necessary libraries, sets a CUDA memory allocation configuration, checks if a GPU is available, and prints relevant GPU information, such as device name, count, and CUDA availability.**",
   "id": "285f2fa0c0196c2d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:36:10.497182Z",
     "start_time": "2025-05-02T08:35:36.243982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import gc\n",
    "import os\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import optuna as ot\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ],
   "id": "0840d80b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Torch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 4070 Ti SUPER\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:36:10.513550Z",
     "start_time": "2025-05-02T08:36:10.510343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ],
   "id": "3bb601c27bf1df1f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:36:10.526688Z",
     "start_time": "2025-05-02T08:36:10.521818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # music_df = pd.read_pickle(\"music_data.pkl\")\n",
    "# # music_df_with_lengths = pd.read_pickle(\"music_data_with_lengths.pkl\")\n",
    "# test_split = pd.read_pickle(\"test_split.pkl\")\n",
    "# train_split = pd.read_pickle(\"train_split.pkl\")\n",
    "# val_split = pd.read_pickle(\"val_split.pkl\")\n",
    "# train_split_with_spec_len = pd.read_pickle(\"train_split_with_spec_len.pkl\")"
   ],
   "id": "9c7b43e66fdc1b6c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:36:10.540773Z",
     "start_time": "2025-05-02T08:36:10.532918Z"
    }
   },
   "cell_type": "code",
   "source": "# train_split_with_spec_len",
   "id": "3ac626b040bf8ace",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:36:11.146005Z",
     "start_time": "2025-05-02T08:36:11.142647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Show the minimum and distribution of token sequence lengths\n",
    "# token_lengths = music_df[\"Encoded_MIDI_Tokens\"].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "# print(\"Minimum token length:\", token_lengths.min())\n",
    "# print(\"Value counts:\")\n",
    "# print(token_lengths.value_counts().sort_index())"
   ],
   "id": "7f0d3dc485959727",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:36:11.159707Z",
     "start_time": "2025-05-02T08:36:11.155617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def is_valid_token_list(x):\n",
    "#     return isinstance(x, list) and len(x) > 0 and all(isinstance(t, (list, tuple)) and len(t) == 4 for t in x)\n",
    "#\n",
    "# bad_token_rows = music_df[~music_df[\"Encoded_MIDI_Tokens\"].apply(is_valid_token_list)]\n",
    "# print(\"🚨 Suspicious rows:\", len(bad_token_rows))\n",
    "# print(bad_token_rows[[\".midi\", \"Encoded_MIDI_Tokens\"]].head())"
   ],
   "id": "201c2fd26297d1fe",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**This code defines two PyTorch classes: `ResidualBlock` and `CNNFeatureSequence`. The `ResidualBlock` implements a residual connection with two convolution layers, while `CNNFeatureSequence` builds a CNN feature extractor using multiple stages of residual blocks, allowing for sequence modeling from input spectrogram data.**",
   "id": "da6d99aa26dcf157"
  },
  {
   "cell_type": "code",
   "id": "a4114f54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:36:11.225722Z",
     "start_time": "2025-05-02T08:36:11.216040Z"
    }
   },
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dilation=1, downsample=None, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Constructs a Residual Block using two 2D convolutions with optional dilation and skip connections.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            dilation (int, optional): Dilation factor for convolutions, controls receptive field. Default is 1.\n",
    "            downsample (callable, optional): Optional 1x1 conv layer to match input/output channels when needed.\n",
    "            dropout (float, optional): Dropout rate applied between convolutions. Default is 0.0 (no dropout).\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        # First convolution layer with dilation\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               padding=dilation, dilation=dilation, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)  # Batch normalization for the first convolution\n",
    "        self.relu = nn.ReLU(inplace=True)  # Activation function\n",
    "        self.dropout = nn.Dropout2d(p=dropout) if dropout > 0 else nn.Identity()  # Dropout for regularization\n",
    "        # Second convolution layer with dilation\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               padding=dilation, dilation=dilation, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)  # Batch normalization for the second convolution\n",
    "        self.downsample = downsample  # Optional layer to match input and output dimensions for skip connections\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the Residual Block.\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [B, C, F, T].\n",
    "        Returns:\n",
    "            Tensor: Output tensor of the same shape as input (unless downsample changes channels).\n",
    "        \"\"\"\n",
    "        identity = x  # Save the original input for the skip connection\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)  # Match input dimensions with output dimensions using the downsample layer\n",
    "        out = self.conv1(x)  # First convolution\n",
    "        out = self.bn1(out)  # Normalize output\n",
    "        out = self.relu(out)  # Apply activation\n",
    "        out = self.dropout(out)  # Apply dropout if any\n",
    "        out = self.conv2(out)  # Second convolution\n",
    "        out = self.bn2(out)  # Normalize again\n",
    "        out += identity  # Add skip connection (residual)\n",
    "        out = self.relu(out)  # Activation after summing\n",
    "        return out\n",
    "\n",
    "class CNNFeatureSequence(nn.Module):\n",
    "    def __init__(self, input_channels=11, feature_dim=512, depths=(6, 6, 6), dropout=0.1):\n",
    "        \"\"\"\n",
    "        Constructs a CNN-based feature extractor for sequence modeling.\n",
    "        Args:\n",
    "            input_channels (int, optional): Number of input channels. Default is 11.\n",
    "            feature_dim (int, optional): Dimensionality of feature outputs. Default is 512.\n",
    "            depths (tuple of int): Number of residual blocks per stage. Default is (6, 6, 6).\n",
    "            dropout (float): Dropout rate applied in residual blocks. Default is 0.1.\n",
    "        \"\"\"\n",
    "        super(CNNFeatureSequence, self).__init__()\n",
    "        # Initial convolutional layer to process the input\n",
    "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=1, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)  # Batch normalization for the initial layer\n",
    "        self.relu = nn.ReLU(inplace=True)  # Activation function\n",
    "        # Pooling layer to downsample the frequency dimension\n",
    "        self.pool_freq = nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))\n",
    "\n",
    "        # Depths of each stage\n",
    "        d1, d2, d3 = depths\n",
    "        # Sequential container for all stages of residual blocks\n",
    "        self.layers = nn.Sequential(\n",
    "            *self._make_layer(64, 128, num_blocks=d1, dilation=1, dropout=dropout),\n",
    "            *self._make_layer(128, 256, num_blocks=d2, dilation=2, dropout=dropout),\n",
    "            *self._make_layer(256, 512, num_blocks=d3, dilation=4, dropout=dropout),\n",
    "        )\n",
    "\n",
    "        # Final projection layer to produce the expected feature dimension\n",
    "        self.projection = nn.Conv2d(512, feature_dim, kernel_size=1)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, dilation, dropout):\n",
    "        \"\"\"\n",
    "        Creates a sequence of residual blocks with optional dilation.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            num_blocks (int): Number of residual blocks.\n",
    "            dilation (int): Dilation factor for convolution layers.\n",
    "            dropout (float): Dropout rate for blocks.\n",
    "        Returns:\n",
    "            list[nn.Module]: A list of residual blocks to be used in a sequential layer.\n",
    "        \"\"\"\n",
    "        # Downsample layer to match input and output dimensions when necessary\n",
    "        downsample = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        ) if in_channels != out_channels else None\n",
    "\n",
    "        # Create the first residual block with downsampling\n",
    "        layers = [ResidualBlock(in_channels, out_channels, dilation=dilation, downsample=downsample, dropout=dropout)]\n",
    "        # Add remaining residual blocks without downsampling\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels, dilation=dilation, dropout=dropout))\n",
    "        return layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the CNN feature extractor.\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [B, C, F, T], where\n",
    "                B = batch size,\n",
    "                C = input channels (e.g. spectrogram types),\n",
    "                F = frequency bins,\n",
    "                T = time frames.\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape [B, T, D * F′], where D is the projected feature size\n",
    "                    and F′ is the downsampled frequency dimension.\n",
    "                    This format is suitable for Transformer input as a time-major sequence.\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)  # Initial convolution\n",
    "        x = self.bn1(x)  # Batch normalization\n",
    "        x = self.relu(x)  # Activation function\n",
    "        x = self.pool_freq(x)  # Downsample frequency dimension\n",
    "        x = self.layers(x)  # Pass through residual blocks\n",
    "        x = self.projection(x)  # Project to feature_dim\n",
    "        x = x.permute(0, 3, 1, 2)  # Rearrange dimensions for sequence modeling (time-major format)\n",
    "        x = x.flatten(2)  # Flatten frequency dimension\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:36:22.264506Z",
     "start_time": "2025-05-02T08:36:11.243734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_split_with_spec_len_df = pd.read_pickle(\"train_split_with_spec_len.pkl\")\n",
    "train_split_with_spec_len_df = train_split_with_spec_len_df.dropna(subset=[\"Spectrogram_Path\"])\n",
    "\n",
    "# Create output directory for CNN features\n",
    "cnn_output_dir = Path(\"cnn_outputs\")\n",
    "os.makedirs(cnn_output_dir, exist_ok=True)\n"
   ],
   "id": "a2152297ad2453db",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:36:22.339209Z",
     "start_time": "2025-05-02T08:36:22.334777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_T = 2048  # Maximum number of time steps per chunk\n",
    "CHUNK_OVERLAP = 256  # Number of overlapping frames between chunks\n",
    "\n",
    "def chunk_tensor(tensor, max_t, overlap=0, pad_short=True):\n",
    "    \"\"\"\n",
    "    Splits an input tensor along the time dimension into chunks.\n",
    "    Args:\n",
    "        tensor (Tensor): Input tensor of shape [1, C, F, T].\n",
    "        max_t (int): Maximum allowed time frames per chunk.\n",
    "        overlap (int): Number of overlapping frames between consecutive chunks.\n",
    "        pad_short (bool): Whether to pad the final chunk if it's shorter than max_t.\n",
    "    Returns:\n",
    "        list[Tensor]: List of [1, C, F, T_chunk] tensors.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    stride = max_t - overlap\n",
    "    _, _, _, total_t = tensor.shape\n",
    "    for start in range(0, total_t, stride):\n",
    "        end = min(start + max_t, total_t)\n",
    "        chunk = tensor[..., start:end]\n",
    "        if chunk.shape[-1] < max_t:\n",
    "            if pad_short:\n",
    "                pad = max_t - chunk.shape[-1]\n",
    "                chunk = F.pad(chunk, (0, pad))  # Pad at the end\n",
    "            else:\n",
    "                break  # Skip if we don't want short tails\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ],
   "id": "449928ef81267038",
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "6adcba63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:36:22.402240Z",
     "start_time": "2025-05-02T08:36:22.397584Z"
    }
   },
   "source": [
    "class MultiHeadOutput(nn.Module):\n",
    "    \"\"\"\n",
    "    Predicts pitch, velocity_bin, duration_bin, and time_bin from transformer outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, pitch_classes=128, velocity_bins=32, duration_bins=64, time_bins=64):\n",
    "        super().__init__()\n",
    "        self.pitch_head = nn.Linear(d_model, pitch_classes)\n",
    "        self.velocity_head = nn.Linear(d_model, velocity_bins)\n",
    "        self.duration_head = nn.Linear(d_model, duration_bins)\n",
    "        self.time_head = nn.Linear(d_model, time_bins)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, D]\n",
    "        return {\n",
    "            \"pitch\": self.pitch_head(x),  # [B, T, pitch_classes]\n",
    "            \"velocity\": self.velocity_head(x),  # [B, T, velocity_bins]\n",
    "            \"duration\": self.duration_head(x),  # [B, T, duration_bins]\n",
    "            \"time\": self.time_head(x),  # [B, T, time_bins]\n",
    "        }\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "b9a68dea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:36:22.466800Z",
     "start_time": "2025-05-02T08:36:22.460830Z"
    }
   },
   "source": [
    "class WAVtoMIDIModel(nn.Module):\n",
    "    def __init__(self, input_channels=11, d_model=512,\n",
    "                 pitch_classes=128, velocity_bins=32, duration_bins=64, time_bins=64):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.cnn = CNNFeatureSequence(input_channels=input_channels, feature_dim=d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=8, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "        self.output_heads = MultiHeadOutput(d_model, pitch_classes, velocity_bins, duration_bins, time_bins)\n",
    "\n",
    "        # Not a module, will be set up later\n",
    "        self._dynamic_projection = None\n",
    "\n",
    "    def _ensure_projection(self, in_dim):\n",
    "        if self._dynamic_projection is None or self._dynamic_projection.in_features != in_dim:\n",
    "            self._dynamic_projection = nn.Linear(in_dim, self.d_model).to(next(self.parameters()).device)\n",
    "            # Register properly for training & saving\n",
    "            self.add_module(\"post_cnn_projection\", self._dynamic_projection)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 4:\n",
    "            # Input is spectrogram: [B, C=11, F, T]\n",
    "            x = self.cnn(x)  # ➜ [B, T, feature_dim]\n",
    "        elif x.dim() == 3:\n",
    "            # Input is precomputed features: [B, T, feature_dim]\n",
    "            pass  # Already suitable for projection\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported input shape: {x.shape}\")\n",
    "\n",
    "        self._ensure_projection(x.shape[-1])\n",
    "        x = self._dynamic_projection(x)\n",
    "        assert x.shape[-1] == self.d_model, f\"❌ Expected projected dim {self.d_model}, got {x.shape[-1]}\"\n",
    "        x = self.transformer(x)\n",
    "        return self.output_heads(x)\n",
    "\n",
    "    def transformer_only(self, features_tensor):\n",
    "        self._ensure_projection(features_tensor.shape[-1])\n",
    "        x = self._dynamic_projection(features_tensor)\n",
    "        return self.transformer(x)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:36:22.529857Z",
     "start_time": "2025-05-02T08:36:22.524224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_loss_classical(predictions, targets, component_weights=None, class_weights=None):\n",
    "    \"\"\"\n",
    "    Compute the total weighted loss for quantized classical music tokens.\n",
    "    Parameters:\n",
    "        predictions (dict): Model outputs with keys [\"pitch\", \"velocity\", \"duration\", \"time\"],\n",
    "            where each tensor has shape [B, T, C].\n",
    "        targets (dict): True labels with the same keys as predictions, each with shape [B, T].\n",
    "        component_weights (dict, optional): Scalar importance weights for each component.\n",
    "            Defaults to {\"pitch\": 1.0, \"velocity\": 0.3, \"duration\": 1.0, \"time\": 1.2}.\n",
    "        class_weights (dict, optional): Dictionary of torch.FloatTensor for per-class weights.\n",
    "            Keys correspond to components.\n",
    "    Returns:\n",
    "        Tuple[Tensor, dict]: Total loss as a tensor and individual component losses as a dictionary.\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    component_losses = {}\n",
    "\n",
    "    if component_weights is None:\n",
    "        component_weights = {\"pitch\": 1.0, \"velocity\": 0.3, \"duration\": 1.0, \"time\": 1.2}\n",
    "\n",
    "    for key in predictions:\n",
    "        pred = predictions[key].reshape(-1, predictions[key].shape[-1])  # [B*T, C]\n",
    "        target = targets[key].reshape(-1)  # [B*T]\n",
    "        C = pred.shape[-1]\n",
    "\n",
    "        # Check for invalid class indices BEFORE calling cross_entropy\n",
    "        if target.max() >= C or target.min() < 0:\n",
    "            print(f\"❌ Invalid class index in {key}: min={target.min().item()}, max={target.max().item()}, num_classes={C}\")\n",
    "            target = torch.clamp(target, 0, C - 1)  # Clamp to valid range to avoid crashing\n",
    "\n",
    "        # Compute loss\n",
    "        if class_weights and key in class_weights:\n",
    "            weight_tensor = class_weights[key].to(pred.device)\n",
    "            loss = F.cross_entropy(pred, target, weight=weight_tensor)\n",
    "        else:\n",
    "            loss = F.cross_entropy(pred, target)\n",
    "\n",
    "        weighted_loss = component_weights.get(key, 1.0) * loss\n",
    "        component_losses[key] = weighted_loss.item()\n",
    "        total_loss += weighted_loss\n",
    "\n",
    "    return total_loss, component_losses"
   ],
   "id": "dba2fdc72dfbbb7c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:36:22.591044Z",
     "start_time": "2025-05-02T08:36:22.587809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def timing_similarity(pred_time, target_time, p=2.0, tau=5.0):\n",
    "    \"\"\"\n",
    "    Computes a soft similarity score between predicted and target timing using a p-norm kernel.\n",
    "    \"\"\"\n",
    "    error = torch.abs(pred_time - target_time).float()\n",
    "    scaled_error = (error / tau).pow(p)\n",
    "    return torch.exp(-scaled_error)\n"
   ],
   "id": "e7fae0d653bc72ce",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:36:22.653813Z",
     "start_time": "2025-05-02T08:36:22.649594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_streak_compensated_score(preds, targets, alpha=1.0, beta=1.0):\n",
    "    \"\"\"\n",
    "    Calculates a score where longer streaks of correct tokens reduce the penalty from mistakes.\n",
    "\n",
    "    Args:\n",
    "        preds (dict): logits for each component, shape [B, T, C]\n",
    "        targets (dict): true class indices, shape [B, T]\n",
    "        alpha (float): reward weight for streak length\n",
    "        beta (float): penalty weight for mistakes\n",
    "\n",
    "    Returns:\n",
    "        float: streak-compensated performance score (higher is better)\n",
    "    \"\"\"\n",
    "    pred_pitch = torch.argmax(preds[\"pitch\"], dim=-1)\n",
    "    pred_velocity = torch.argmax(preds[\"velocity\"], dim=-1)\n",
    "    pred_duration = torch.argmax(preds[\"duration\"], dim=-1)\n",
    "    pred_time = torch.argmax(preds[\"time\"], dim=-1)\n",
    "\n",
    "    all_correct = (\n",
    "        (pred_pitch == targets[\"pitch\"]) &\n",
    "        (pred_velocity == targets[\"velocity\"]) &\n",
    "        (pred_duration == targets[\"duration\"]) &\n",
    "        (pred_time == targets[\"time\"])\n",
    "    ).int()  # [B, T]\n",
    "\n",
    "    scores = []\n",
    "    for sequence in all_correct:\n",
    "        streak = 0\n",
    "        streak_sum = 0\n",
    "        error_count = 0\n",
    "        for token in sequence:\n",
    "            if token:\n",
    "                streak += 1\n",
    "                streak_sum += streak  # reward increases with streak length\n",
    "            else:\n",
    "                error_count += 1\n",
    "                streak = 0\n",
    "        N = len(sequence)\n",
    "        reward = alpha * (streak_sum / N)\n",
    "        penalty = beta * (error_count / N)\n",
    "        scores.append(reward - penalty)\n",
    "\n",
    "    return sum(scores) / len(scores)"
   ],
   "id": "41e5b1ae8c4e9783",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:36:22.716238Z",
     "start_time": "2025-05-02T08:36:22.711791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_soft_streak_score(preds, targets, alpha=1.0, beta=1.0, p=2.0, tau=5.0):\n",
    "    pred_pitch = torch.argmax(preds[\"pitch\"], dim=-1)\n",
    "    pred_velocity = torch.argmax(preds[\"velocity\"], dim=-1)\n",
    "    pred_duration = torch.argmax(preds[\"duration\"], dim=-1)\n",
    "    pred_time = torch.argmax(preds[\"time\"], dim=-1)\n",
    "\n",
    "    pitch_correct = (pred_pitch == targets[\"pitch\"]).float()\n",
    "    velocity_correct = (pred_velocity == targets[\"velocity\"]).float()\n",
    "    duration_correct = (pred_duration == targets[\"duration\"]).float()\n",
    "    time_score = timing_similarity(pred_time, targets[\"time\"], p=p, tau=tau)\n",
    "\n",
    "    token_scores = pitch_correct * velocity_correct * duration_correct * time_score\n",
    "\n",
    "    scores = []\n",
    "    for sequence in token_scores:\n",
    "        streak = 0.0\n",
    "        streak_score = 0.0\n",
    "        for token_score in sequence:\n",
    "            if token_score > 0.5:  # Optional softness threshold\n",
    "                streak += 1\n",
    "                streak_score += streak\n",
    "            else:\n",
    "                streak = 0\n",
    "        scores.append(streak_score / len(sequence))\n",
    "\n",
    "    return alpha * torch.tensor(scores).mean().item() - beta * (1 - torch.tensor(scores).mean().item())\n"
   ],
   "id": "ee1564d51bd75a9f",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:36:22.778935Z",
     "start_time": "2025-05-02T08:36:22.773494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def collate_fn(batch):\n",
    "    # Filter out bad samples and keep track of original indices\n",
    "    batch = [(i, b) for i, b in enumerate(batch) if b is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None, None\n",
    "\n",
    "    indices, valid_batch = zip(*batch)\n",
    "    spectrograms, midi_tokens = zip(*valid_batch)\n",
    "\n",
    "    # Filter out spectrograms with incorrect channel count\n",
    "    filtered = []\n",
    "    for i, (spec, tokens) in enumerate(zip(spectrograms, midi_tokens)):\n",
    "        if spec.shape[0] != 11:\n",
    "            print(f\"❌ Spectrogram at idx {indices[i]} has shape {spec.shape}, expected 11 channels\")\n",
    "            continue\n",
    "        filtered.append((spec, tokens))\n",
    "\n",
    "    if len(filtered) == 0:\n",
    "        print(\"⚠️ All spectrograms had invalid channel counts\")\n",
    "        return None, None\n",
    "\n",
    "    spectrograms, midi_tokens = zip(*filtered)\n",
    "\n",
    "    # Stack spectrograms and pad MIDI tokens\n",
    "    midi_tokens = [torch.tensor(seq, dtype=torch.long) for seq in midi_tokens]\n",
    "    midi_tokens_padded = pad_sequence(midi_tokens, batch_first=True, padding_value=0)\n",
    "\n",
    "    try:\n",
    "        spectrograms = torch.stack(spectrograms)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error stacking spectrograms: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    print(\"\\n📦 Batch sizes:\")\n",
    "    for i, tok in enumerate(midi_tokens):\n",
    "        print(f\"  ▸ Token {i}: shape={tok.shape}\")\n",
    "    print(f\"Spectrogram shapes: {[s.shape for s in spectrograms]}\\n\")\n",
    "\n",
    "    return spectrograms, midi_tokens_padded"
   ],
   "id": "87d8b7c216e4caaa",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:36:22.873602Z",
     "start_time": "2025-05-02T08:36:22.862385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Quantizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        velocity_bins=32,\n",
    "        duration_bins=64,\n",
    "        duration_range=(0.01, 10.0),\n",
    "        time_bins=32,\n",
    "        time_range=(0.01, 8.0),\n",
    "    ):\n",
    "        self.velocity_bins = velocity_bins\n",
    "        self.duration_bins = duration_bins\n",
    "        self.time_bins = time_bins\n",
    "\n",
    "        # Velocity edges are implicit (fixed-width)\n",
    "        self.velocity_bin_size = 128 / velocity_bins\n",
    "\n",
    "        # Log-space bins for duration and time\n",
    "        self.duration_edges = np.logspace(np.log10(duration_range[0]), np.log10(duration_range[1]), num=duration_bins + 1)\n",
    "        self.time_edges = np.logspace(np.log10(time_range[0]), np.log10(time_range[1]), num=time_bins + 1)\n",
    "\n",
    "    def quantize_velocity(self, velocity):\n",
    "        \"\"\"Quantize velocity [0–127] into bins.\"\"\"\n",
    "        bin_idx = int(velocity // self.velocity_bin_size)\n",
    "        return min(bin_idx, self.velocity_bins - 1)\n",
    "\n",
    "    def quantize_duration(self, duration_sec):\n",
    "        \"\"\"Quantize duration (in seconds) into log-spaced bins.\"\"\"\n",
    "        bin_idx = np.digitize(duration_sec, self.duration_edges) - 1\n",
    "        return int(np.clip(bin_idx, 0, self.duration_bins - 1))\n",
    "\n",
    "    def quantize_time(self, time_sec):\n",
    "        \"\"\"Quantize time since the last note (in seconds) into log-spaced bins.\"\"\"\n",
    "        bin_idx = np.digitize(time_sec, self.time_edges) - 1\n",
    "        return int(np.clip(bin_idx, 0, self.time_bins - 1))\n",
    "\n",
    "    def inverse_velocity(self, bin_idx):\n",
    "        \"\"\"Approximate original velocity from bin.\"\"\"\n",
    "        return int((bin_idx + 0.5) * self.velocity_bin_size)\n",
    "\n",
    "    def inverse_duration(self, bin_idx):\n",
    "        \"\"\"Approximate original duration (seconds) from bin.\"\"\"\n",
    "        bin_idx = np.clip(bin_idx, 0, self.duration_bins - 1)\n",
    "        return float((self.duration_edges[bin_idx] + self.duration_edges[bin_idx + 1]) / 2)\n",
    "\n",
    "    def inverse_time(self, bin_idx):\n",
    "        \"\"\"Approximate original time gap (seconds) from bin.\"\"\"\n",
    "        bin_idx = np.clip(bin_idx, 0, self.time_bins - 1)\n",
    "        return float((self.time_edges[bin_idx] + self.time_edges[bin_idx + 1]) / 2)"
   ],
   "id": "e20949676b1299e5",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:36:22.947848Z",
     "start_time": "2025-05-02T08:36:22.944030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "quantizer = Quantizer(\n",
    "    velocity_bins=32,\n",
    "    duration_bins=64,\n",
    "    time_bins=64,\n",
    "    duration_range=(0.01, 5.0),   # reasonable bounds from distribution\n",
    "    time_range=(0.01, 5.0),       # ditto for time since last note\n",
    ")\n"
   ],
   "id": "423d31b6fd6f1ad9",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**This code defines a `MaestroDataset` class, a PyTorch `Dataset` for loading and preprocessing data for MIDI and spectrogram files.**\n",
    "\n",
    "- It drops rows with missing `Spectrogram_Path` or `Encoded_MIDI_Tokens`.\n",
    "- In `__getitem__`, it:\n",
    "  - Loads and validates the spectrogram file.\n",
    "  - Fixes spectrogram shape issues if needed.\n",
    "  - Validates the structure of `Encoded_MIDI_Tokens`.\n",
    "  - Returns processed `spectrogram` and `midi_tokens` as tensors.\n",
    "\n",
    "Invalid data is logged and skipped."
   ],
   "id": "3118c3cffd23f63d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:36:23.016338Z",
     "start_time": "2025-05-02T08:36:23.010193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MaestroDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        # Drop rows with missing spectrogram paths or encoded MIDI tokens\n",
    "        self.df = df.dropna(subset=[\"Spectrogram_Path\", \"Encoded_MIDI_Tokens\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        spectrogram_path = row[\"Spectrogram_Path\"]\n",
    "\n",
    "        if not spectrogram_path or not os.path.exists(spectrogram_path):\n",
    "            raise IndexError(f\"Missing spectrogram file at index {idx}: {spectrogram_path}\")\n",
    "\n",
    "        try:\n",
    "            spectrogram = torch.load(spectrogram_path, map_location=\"cpu\")\n",
    "\n",
    "            if not isinstance(spectrogram, torch.Tensor):\n",
    "                spectrogram = torch.tensor(spectrogram, dtype=torch.float32)\n",
    "\n",
    "            # Ensure spectrogram shape is [11, F, T]\n",
    "            if spectrogram.ndim == 3 and spectrogram.shape[0] == 11:\n",
    "                pass\n",
    "            elif spectrogram.ndim == 2 and spectrogram.shape[0] == 11:\n",
    "                spectrogram = spectrogram.unsqueeze(0)\n",
    "            elif spectrogram.ndim == 3 and spectrogram.shape[1] == 11:\n",
    "                spectrogram = spectrogram.permute(1, 0, 2)\n",
    "            else:\n",
    "                raise IndexError(f\"Invalid spectrogram shape at index {idx}: {spectrogram.shape}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            raise IndexError(f\"Error loading spectrogram at index {idx}: {e}\")\n",
    "\n",
    "        # Load and validate MIDI tokens\n",
    "        midi_tokens = row[\"Encoded_MIDI_Tokens\"]\n",
    "        if not isinstance(midi_tokens, list) or len(midi_tokens) == 0:\n",
    "            raise IndexError(f\"Empty or invalid MIDI token list at index {idx}\")\n",
    "\n",
    "        if not all(isinstance(t, (list, tuple)) and len(t) == 4 for t in midi_tokens):\n",
    "            raise IndexError(f\"Malformed MIDI token structure at index {idx}: {midi_tokens[:3]}\")\n",
    "\n",
    "        # Quantize tokens\n",
    "        quantized_tokens = []\n",
    "        for pitch, velocity, duration, time in midi_tokens:\n",
    "            velocity_bin = quantizer.quantize_velocity(velocity)\n",
    "            duration_bin = quantizer.quantize_duration(duration)\n",
    "            time_bin = quantizer.quantize_time(time)\n",
    "            quantized_tokens.append((pitch, velocity_bin, duration_bin, time_bin))\n",
    "\n",
    "        midi_tokens = torch.tensor(quantized_tokens, dtype=torch.long)\n",
    "\n",
    "        if midi_tokens.ndim != 2 or midi_tokens.shape[1] != 4:\n",
    "            raise IndexError(f\"Unexpected MIDI tensor shape at index {idx}: {midi_tokens.shape}\")\n",
    "\n",
    "        return spectrogram, midi_tokens"
   ],
   "id": "eb49bbf95a98d90c",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:36:23.076821Z",
     "start_time": "2025-05-02T08:36:23.072541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = MaestroDataset(train_split_with_spec_len_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)"
   ],
   "id": "cb6c759b66b51925",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**This cell defines the `objective` function for an Optuna study to optimize hyperparameters for a model converting WAV to MIDI.**\n",
    "\n",
    "Key steps:\n",
    "1. **Clean Resources**: Frees GPU/CPU memory before training.\n",
    "2. **Hyperparameter Sampling**: Samples CNN depths, dropout, transformer layers, number of heads, and learning rate.\n",
    "3. **Model Setup**: Configures the model, transformer encoder, and optimizer with sampled hyperparameters.\n",
    "4. **Training Loop**: Processes batches - applies chunking if spectrograms are too long, computes predictions, loss, and performs backpropagation.\n",
    "5. **Streak Evaluation**: Computes soft streak scores for performance feedback.\n",
    "6. **Plateau Handling**: Adjusts penalty (`p`) if performance improvement stalls beyond a patience threshold.\n",
    "7. **Result Output**: Returns the negative mean score for minimization by Optuna."
   ],
   "id": "43922d92b1d5cac6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:36:23.177067Z",
     "start_time": "2025-05-02T08:36:23.134083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plateau_count = 0\n",
    "best_score = float(\"-inf\")\n",
    "p = 1.0  # Start soft\n",
    "delta_p = 0.5  # Penalty increment value\n",
    "max_p = 4.0  # Maximum penalty weight\n",
    "patience = 15  # Trials without significant improvement before increasing penalty\n",
    "\n",
    "# Store the best model per trial\n",
    "best_model_state = None\n",
    "best_trial_score = float(\"-inf\")\n",
    "\n",
    "use_fine_timing = False  # Start with snapped timing, refine later\n",
    "\n",
    "MAX_T = 1048  # Lowered max time length for chunking\n",
    "CHUNK_OVERLAP = 128\n",
    "MEMORY_THRESHOLD = 0.8  # 70% of combined memory (GPU + CPU fallback)\n",
    "\n",
    "import pynvml\n",
    "import psutil\n",
    "pynvml.nvmlInit()\n",
    "gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "def total_memory_bytes():\n",
    "    meminfo = pynvml.nvmlDeviceGetMemoryInfo(gpu_handle)\n",
    "    return meminfo.total + psutil.virtual_memory().total\n",
    "\n",
    "def used_memory_bytes():\n",
    "    meminfo = pynvml.nvmlDeviceGetMemoryInfo(gpu_handle)\n",
    "    return meminfo.used + psutil.virtual_memory().used\n",
    "\n",
    "def gpu_memory_fraction():\n",
    "    total = total_memory_bytes()\n",
    "    used = used_memory_bytes()\n",
    "    return used / total if total else 0\n",
    "\n",
    "def objective(trial):\n",
    "    global best_score, plateau_count, p, best_model_state, best_trial_score, use_fine_timing\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Sample hyperparameters\n",
    "    depths = (\n",
    "        trial.suggest_int(\"depth_stage1\", 2, 6),\n",
    "        trial.suggest_int(\"depth_stage2\", 2, 6),\n",
    "        trial.suggest_int(\"depth_stage3\", 2, 6),\n",
    "    )\n",
    "    dropout = 0.0  # 🔒 Force dropout off to save memory\n",
    "    num_layers = trial.suggest_int(\"num_transformer_layers\", 2, 8)\n",
    "    nhead = trial.suggest_categorical(\"nhead\", [4, 8, 16])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "\n",
    "    model = WAVtoMIDIModel(input_channels=11, d_model=512).to(device)\n",
    "    encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=nhead, batch_first=True)\n",
    "    model.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "\n",
    "    from torch.utils.checkpoint import checkpoint_sequential\n",
    "    model.cnn.layers.forward = lambda x: checkpoint_sequential(model.cnn.layers, segments=3, input=x, use_reentrant=False)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        if batch is None:\n",
    "            continue\n",
    "\n",
    "        spectrograms, midi_tokens = batch\n",
    "        t_lengths = [s.shape[-1] for s in spectrograms]\n",
    "        print(f\"\\U0001f9ea Spectrogram time lengths → min: {min(t_lengths)}, max: {max(t_lengths)}, avg: {sum(t_lengths) / len(t_lengths):.2f}\")\n",
    "        spectrograms = spectrograms.to(device)\n",
    "        midi_tokens = midi_tokens.to(device)\n",
    "\n",
    "        try:\n",
    "            targets = {\n",
    "                \"pitch\": midi_tokens[..., 0],\n",
    "                \"velocity\": midi_tokens[..., 1],\n",
    "                \"duration\": midi_tokens[..., 2],\n",
    "                \"time\": midi_tokens[..., 3],\n",
    "            }\n",
    "        except IndexError:\n",
    "            print(f\"⚠️ Bad token shape: {midi_tokens.shape}\")\n",
    "            return float(\"inf\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if spectrograms.shape[-1] > MAX_T:\n",
    "            chunks = chunk_tensor(spectrograms[0].unsqueeze(0), MAX_T, overlap=CHUNK_OVERLAP, pad_short=True)\n",
    "            all_preds = []\n",
    "            accumulated_tokens = []\n",
    "            for chunk in chunks:\n",
    "                current_fraction = gpu_memory_fraction()\n",
    "                if current_fraction >= MEMORY_THRESHOLD:\n",
    "                    print(f\"🚨 Combined memory usage exceeded {MEMORY_THRESHOLD*100:.0f}% ({current_fraction:.2f}), stopping chunk loading early.\")\n",
    "                    break\n",
    "                print(f\"🚦 Chunk shape: {chunk.shape}\")\n",
    "                chunk = chunk.to(device)\n",
    "                chunk = chunk.float()\n",
    "                feat = model.cnn(chunk)\n",
    "                if feat.dim() == 2:\n",
    "                    feat = feat.unsqueeze(0)\n",
    "                out = model.transformer_only(feat)\n",
    "                out = model.output_heads(out)\n",
    "                all_preds.append(out)\n",
    "                accumulated_tokens.append(MAX_T)\n",
    "                del chunk, feat, out\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            if not all_preds:\n",
    "                print(\"❌ No chunks processed before hitting memory limit.\")\n",
    "                # return float(\"inf\")\n",
    "                continue\n",
    "            final_preds = {}\n",
    "            for key in all_preds[0]:\n",
    "                final_preds[key] = torch.cat([p[key] for p in all_preds], dim=1)\n",
    "            preds = final_preds\n",
    "\n",
    "            # Truncate targets to match final prediction length\n",
    "            # chunked_length = sum(accumulated_tokens)\n",
    "            chunked_length = final_preds[\"pitch\"].shape[1]\n",
    "\n",
    "            for key in targets:\n",
    "                targets[key] = targets[key][:, :chunked_length]\n",
    "        else:\n",
    "            preds = model(spectrograms)\n",
    "\n",
    "        target_len = targets[\"pitch\"].shape[1]\n",
    "        for key in preds:\n",
    "            preds[key] = preds[key][:, :target_len, :]\n",
    "\n",
    "        if use_fine_timing:\n",
    "            print(\"🎯 Using fine timing targets\")\n",
    "\n",
    "        loss, _ = compute_loss_classical(preds, targets)\n",
    "        assert loss.requires_grad, \"Loss does not require grad — check no_grad() usage\"\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            score = compute_soft_streak_score(preds, targets, p=p)\n",
    "            scores.append(score)\n",
    "\n",
    "        if i >= 5:\n",
    "            break\n",
    "\n",
    "    mean_score = sum(scores) / len(scores)\n",
    "    improvement = mean_score - best_score\n",
    "\n",
    "    if improvement < 0.01:\n",
    "        plateau_count += 1\n",
    "    else:\n",
    "        best_score = mean_score\n",
    "        plateau_count = 0\n",
    "\n",
    "    if plateau_count >= patience and p < max_p:\n",
    "        p = min(max_p, p + delta_p)\n",
    "        print(f\"⏫ Increasing streak penalty: p → {p}\")\n",
    "        plateau_count = 0\n",
    "        use_fine_timing = True\n",
    "\n",
    "    if mean_score > best_trial_score:\n",
    "        best_trial_score = mean_score\n",
    "        best_model_state = model.state_dict()\n",
    "\n",
    "    return -mean_score"
   ],
   "id": "1944e69323bd158",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "study = ot.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100)"
   ],
   "id": "c5401165c5aa6f21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "storage_path = \"sqlite:///optuna_study_music_model.db\"\n",
    "study_name = \"music_model_tuning\"\n",
    "\n",
    "# Create a persistent study and migrate the trials\n",
    "persistent_study = ot.create_study(\n",
    "    direction=study.direction,\n",
    "    study_name=study_name,\n",
    "    storage=storage_path,\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "# Copy all trials from the in-memory study\n",
    "for trial in study.trials:\n",
    "    persistent_study.add_trial(trial)\n",
    "\n",
    "print(f\"✅ Study saved to: {storage_path}\")"
   ],
   "id": "66052fd4868b1f0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:36:28.404370Z",
     "start_time": "2025-05-02T08:36:23.235711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "study = ot.load_study(\n",
    "    study_name=\"music_model_tuning\",\n",
    "    storage=\"sqlite:///optuna_study_music_model.db\"\n",
    ")\n",
    "\n",
    "print(study.best_trial)"
   ],
   "id": "732ad398fb0a9f91",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenTrial(number=22, state=1, values=[0.9977099236566573], datetime_start=datetime.datetime(2025, 4, 20, 21, 36, 50, 758089), datetime_complete=datetime.datetime(2025, 4, 20, 22, 4, 20, 213042), params={'depth_stage1': 5, 'depth_stage2': 3, 'depth_stage3': 4, 'num_transformer_layers': 6, 'nhead': 4, 'lr': 0.0006127182157069003}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'depth_stage1': IntDistribution(high=6, log=False, low=2, step=1), 'depth_stage2': IntDistribution(high=6, log=False, low=2, step=1), 'depth_stage3': IntDistribution(high=6, log=False, low=2, step=1), 'num_transformer_layers': IntDistribution(high=8, log=False, low=2, step=1), 'nhead': CategoricalDistribution(choices=(4, 8, 16)), 'lr': FloatDistribution(high=0.001, log=True, low=1e-05, step=None)}, trial_id=23, value=None)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:36:28.472605Z",
     "start_time": "2025-05-02T08:36:28.464599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_params = study.best_params\n",
    "print(\"🔧 Best hyperparameters:\", best_params)"
   ],
   "id": "5221cf53379647b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Best hyperparameters: {'depth_stage1': 5, 'depth_stage2': 3, 'depth_stage3': 4, 'num_transformer_layers': 6, 'nhead': 4, 'lr': 0.0006127182157069003}\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:36:46.128249Z",
     "start_time": "2025-05-02T08:36:28.532853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Unpack the best parameters\n",
    "depths = (\n",
    "    best_params[\"depth_stage1\"],\n",
    "    best_params[\"depth_stage2\"],\n",
    "    best_params[\"depth_stage3\"]\n",
    ")\n",
    "num_layers = best_params[\"num_transformer_layers\"]\n",
    "nhead = best_params[\"nhead\"]\n",
    "lr = best_params[\"lr\"]\n",
    "\n",
    "# Create model\n",
    "model = WAVtoMIDIModel(input_channels=11, d_model=512).to(device)\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=nhead, batch_first=True)\n",
    "model.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ],
   "id": "a68704bc3ad2908e",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T08:36:46.195818Z",
     "start_time": "2025-05-02T08:36:46.191517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_tensor_safely(path):\n",
    "    \"\"\"\n",
    "    Safely loads a tensor from a file, validating contents and handling potential errors.\n",
    "\n",
    "    Args:\n",
    "        path (str or Path): Path to the tensor file to load.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Loaded tensor if successful.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If torch.load fails or the result is not a valid tensor.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import os\n",
    "\n",
    "    path = str(path)\n",
    "    if not os.path.isfile(path):\n",
    "        raise FileNotFoundError(f\"❌ File not found: {path}\")\n",
    "\n",
    "    try:\n",
    "        tensor = torch.load(path, map_location=\"cuda\")\n",
    "        if not isinstance(tensor, torch.Tensor):\n",
    "            raise TypeError(f\"❌ Loaded object is not a torch.Tensor: {type(tensor)} from {path}\")\n",
    "        if tensor.ndim < 2 or not torch.isfinite(tensor).all():\n",
    "            raise ValueError(f\"❌ Loaded tensor is invalid (non-finite or too few dimensions): {tensor.shape}\")\n",
    "        return tensor\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load tensor from {path}: {e}\")\n",
    "        raise"
   ],
   "id": "c301ae1dee726172",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ce7a07a8d62b3012"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-02T08:36:46.265291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize model\n",
    "best_depths = depths\n",
    "model = CNNFeatureSequence(\n",
    "    input_channels=11,\n",
    "    feature_dim=512,\n",
    "    depths=best_depths,\n",
    "    dropout=0.0\n",
    ").to(device)\n",
    "model.eval()\n",
    "\n",
    "cnn_feature_paths = []\n",
    "\n",
    "# Process and save CNN outputs\n",
    "for idx in range(len(train_split_with_spec_len_df)):\n",
    "    row = train_split_with_spec_len_df.iloc[idx]\n",
    "    row_index = train_split_with_spec_len_df.index[idx]\n",
    "    feature_path = cnn_output_dir / f\"cnn_output_{row_index}.pt\"\n",
    "\n",
    "    if feature_path.exists():\n",
    "        train_split_with_spec_len_df.loc[row_index, \"CNN_Feature_Path\"] = str(feature_path)\n",
    "        continue\n",
    "\n",
    "    print(f\"{row_index}\")\n",
    "    try:\n",
    "        spec = load_tensor_safely(row[\"Spectrogram_Path\"])\n",
    "        if not isinstance(spec, torch.Tensor):\n",
    "            spec = torch.tensor(spec, dtype=torch.float32)\n",
    "        spec = spec.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if spec.shape[-1] > MAX_T:\n",
    "                chunks = chunk_tensor(spec, MAX_T, overlap=CHUNK_OVERLAP)\n",
    "                outputs = [model(chunk).squeeze(0).cpu() for chunk in chunks]\n",
    "                output = torch.cat(outputs, dim=0)\n",
    "            else:\n",
    "                output = model(spec).squeeze(0).cpu()\n",
    "\n",
    "        torch.save(output, feature_path)\n",
    "        train_split_with_spec_len_df.loc[row_index, \"CNN_Feature_Path\"] = str(feature_path)\n",
    "\n",
    "        del spec, output\n",
    "        gc.collect()\n",
    "        print(f\"RAM used: {psutil.virtual_memory().used / 1e9:.2f} GB\")\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            train_split_with_spec_len_df.to_pickle(\"train_split_with_cnn_paths_progress.pkl\")\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(f\"⚠️ RuntimeError at index {row_index}: {e}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        train_split_with_spec_len_df.loc[row_index, \"CNN_Feature_Path\"] = None\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Unexpected error at index {row_index}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        train_split_with_spec_len_df.loc[row_index, \"CNN_Feature_Path\"] = None\n",
    "\n",
    "print(\"✅ Done.\")"
   ],
   "id": "30169f977ba13044",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIDI-Unprocessed_04_R1_2011_MID--AUDIO_R1-D2_02_Track02_wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Logan\\AppData\\Local\\Temp\\ipykernel_11828\\1223260482.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tensor = torch.load(path, map_location=\"cuda\")\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Training Loop Using Saved CNN Features**\n",
    "\n",
    "This cell defines a function `train_full_model_with_saved_features()` that trains a Transformer-based WAV-to-MIDI model using **precomputed CNN outputs** saved to disk, rather than recomputing CNN features in real-time.\n",
    "\n",
    "**Function Overview:**\n",
    "- **Input:**\n",
    "  - A PyTorch model that includes a Transformer encoder and multi-head output heads.\n",
    "  - A DataFrame with:\n",
    "    - `CNN_Feature_Path`: Path to the saved CNN feature tensor for each sample.\n",
    "    - `Encoded_MIDI_Tokens`: Quantized MIDI tokens as target labels.\n",
    "  - Number of epochs to train.\n",
    "  - Output directory for model checkpoints.\n",
    "\n",
    "**What it does:**\n",
    "1. **Loads CNN feature tensors** and corresponding MIDI tokens.\n",
    "2. **Validates** that each feature and token pair is shaped correctly.\n",
    "3. **Feeds features into the Transformer**, skipping the CNN stage.\n",
    "4. **Computes prediction loss** using a custom multi-head CrossEntropy setup.\n",
    "5. **Tracks average loss and score** for each epoch using a streak-based evaluation metric.\n",
    "6. **Saves checkpoints** after every epoch, and retains the best-scoring model.\n",
    "\n",
    "This structure avoids recomputing spectrogram CNNs during training, which greatly speeds up experiments and reduces GPU/CPU memory usage."
   ],
   "id": "15dafc7c722adf73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_full_model(\n",
    "        model: torch.nn.Module,\n",
    "        df: pd.DataFrame,\n",
    "        epochs: int = 10,\n",
    "        checkpoint_dir: str = \"checkpoints\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Trains a given model on a dataset containing CNN features and MIDI token data over multiple epochs. The\n",
    "    training includes preprocessing of input data, forward passes, loss computation, backward propagation,\n",
    "    and optimizer updates. It records training statistics such as epoch losses and scores, and saves model\n",
    "    checkpoints during the training process. The best-performing model, based on evaluation scores, is\n",
    "    persisted separately.\n",
    "\n",
    "    Params:\n",
    "    :param model: The PyTorch neural network model to be trained.\n",
    "    :type model: torch.nn.Module\n",
    "    :param df: DataFrame containing training data, with columns \"CNN_Feature_Path\" and \"Encoded_MIDI_Tokens\".\n",
    "    :type df: pd.DataFrame\n",
    "    :param epochs: The number of epochs for training. Defaults to 10.\n",
    "    :type epochs: int, optional\n",
    "    :param checkpoint_dir: Directory path where checkpoint and best model files will be saved. Defaults to \"checkpoints\".\n",
    "    :type checkpoint_dir: str, optional\n",
    "\n",
    "    Return:\n",
    "    :return: This function does not return any value. It performs training as a side effect.\n",
    "    :rtype: None\n",
    "    \"\"\"\n",
    "\n",
    "    def gpu_memory_fraction():\n",
    "        try:\n",
    "            pynvml.nvmlInit()\n",
    "            handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "            gpu_mem = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "            cpu_mem = psutil.virtual_memory()\n",
    "            total = gpu_mem.total + cpu_mem.total\n",
    "            used = gpu_mem.used + cpu_mem.used\n",
    "            return used / total if total else 0\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not read GPU memory stats: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    from torch.optim import Adam\n",
    "\n",
    "    model.train()\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    best_score = float(\"-inf\")\n",
    "    global p\n",
    "    p = 2.0\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        score_sum = 0.0\n",
    "        count = 0\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            try:\n",
    "                feat_path = row.get(\"CNN_Feature_Path\")\n",
    "                tokens = row.get(\"Encoded_MIDI_Tokens\")\n",
    "\n",
    "                if not feat_path or not os.path.exists(feat_path):\n",
    "                    print(f\"❌ Missing feature path for index {idx}: {feat_path}\")\n",
    "                    continue\n",
    "                if not isinstance(tokens, list) or len(tokens) == 0:\n",
    "                    print(f\"⚠️ Invalid or empty MIDI tokens at index {idx}\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    feat = torch.load(feat_path, map_location=\"cpu\")\n",
    "                    if feat.ndim == 2:\n",
    "                        feat = feat.unsqueeze(0)  # Add batch dimension\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Error loading CNN features at {feat_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                midi_tensor = torch.tensor(tokens, dtype=torch.long)\n",
    "                if midi_tensor.ndim != 2 or midi_tensor.shape[1] != 4:\n",
    "                    print(f\"❌ Bad MIDI token shape at index {idx}: {midi_tensor.shape}\")\n",
    "                    continue\n",
    "\n",
    "                feat, midi_tensor = feat.to(device), midi_tensor.to(device)\n",
    "                targets = {\n",
    "                    \"pitch\": midi_tensor[:, 0].unsqueeze(0),\n",
    "                    \"velocity\": midi_tensor[:, 1].unsqueeze(0),\n",
    "                    \"duration\": midi_tensor[:, 2].unsqueeze(0),\n",
    "                    \"time\": midi_tensor[:, 3].unsqueeze(0),\n",
    "                }\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                preds = model(feat)\n",
    "\n",
    "                for key in preds:\n",
    "                    T_pred = preds[key].shape[1]\n",
    "                    T_target = targets[key].shape[1]\n",
    "                    min_T = min(T_pred, T_target)\n",
    "                    preds[key] = preds[key][:, :min_T, :]\n",
    "                    targets[key] = targets[key][:, :min_T]\n",
    "\n",
    "                loss, _ = compute_loss_classical(preds, targets)\n",
    "                if not loss.requires_grad:\n",
    "                    print(f\"⚠️ Skipping index {idx} due to non-grad loss\")\n",
    "                    continue\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    score = compute_soft_streak_score(preds, targets, p=p)\n",
    "                    score_sum += score\n",
    "                    total_loss += loss.item()\n",
    "                    count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"🛑 Error in training at index {idx}: {e}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "        if count == 0:\n",
    "            print(\"⚠️ No valid samples this epoch\")\n",
    "            continue\n",
    "\n",
    "        avg_loss = total_loss / count\n",
    "        avg_score = score_sum / count\n",
    "\n",
    "        print(f\"📚 Epoch {epoch+1}/{epochs} — Loss: {avg_loss:.4f}, Score: {avg_score:.4f}\")\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"epoch_{epoch+1:03d}_score_{avg_score:.4f}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "            'score': avg_score\n",
    "        }, checkpoint_path)\n",
    "\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            best_path = os.path.join(checkpoint_dir, \"best_model.pt\")\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print(f\"💾 Best model updated: {best_path}\")"
   ],
   "id": "db31561d6ba1fb53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_full_model(model, train_loader, epochs=20, use_fine_timing=True)",
   "id": "3912c32f5517892a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "89dad5536b9d1291",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Music_AI)",
   "language": "python",
   "name": "music_ai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
