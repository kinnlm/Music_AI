{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**This code initializes the environment for a PyTorch-based project. It imports necessary libraries, sets a CUDA memory allocation configuration, checks if a GPU is available, and prints relevant GPU information, such as device name, count, and CUDA availability.**",
   "id": "285f2fa0c0196c2d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T22:20:07.507663Z",
     "start_time": "2025-04-24T22:19:58.172701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc  # Garbage collection to free memory\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import optuna as ot\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.fx import traceback\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ],
   "id": "0840d80b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Torch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 4070 Ti SUPER\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T22:20:07.822379Z",
     "start_time": "2025-04-24T22:20:07.816496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ],
   "id": "3bb601c27bf1df1f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T22:20:07.840473Z",
     "start_time": "2025-04-24T22:20:07.836390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # music_df = pd.read_pickle(\"music_data.pkl\")\n",
    "# # music_df_with_lengths = pd.read_pickle(\"music_data_with_lengths.pkl\")\n",
    "# test_split = pd.read_pickle(\"test_split.pkl\")\n",
    "# train_split = pd.read_pickle(\"train_split.pkl\")\n",
    "# val_split = pd.read_pickle(\"val_split.pkl\")\n",
    "# train_split_with_spec_len = pd.read_pickle(\"train_split_with_spec_len.pkl\")"
   ],
   "id": "9c7b43e66fdc1b6c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T22:20:07.855917Z",
     "start_time": "2025-04-24T22:20:07.853343Z"
    }
   },
   "cell_type": "code",
   "source": "# train_split_with_spec_len",
   "id": "3ac626b040bf8ace",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T22:20:07.870560Z",
     "start_time": "2025-04-24T22:20:07.866516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Show the minimum and distribution of token sequence lengths\n",
    "# token_lengths = music_df[\"Encoded_MIDI_Tokens\"].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "# print(\"Minimum token length:\", token_lengths.min())\n",
    "# print(\"Value counts:\")\n",
    "# print(token_lengths.value_counts().sort_index())"
   ],
   "id": "7f0d3dc485959727",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T22:20:07.886454Z",
     "start_time": "2025-04-24T22:20:07.882997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def is_valid_token_list(x):\n",
    "#     return isinstance(x, list) and len(x) > 0 and all(isinstance(t, (list, tuple)) and len(t) == 4 for t in x)\n",
    "#\n",
    "# bad_token_rows = music_df[~music_df[\"Encoded_MIDI_Tokens\"].apply(is_valid_token_list)]\n",
    "# print(\"🚨 Suspicious rows:\", len(bad_token_rows))\n",
    "# print(bad_token_rows[[\".midi\", \"Encoded_MIDI_Tokens\"]].head())"
   ],
   "id": "201c2fd26297d1fe",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**This code defines two PyTorch classes: `ResidualBlock` and `CNNFeatureSequence`. The `ResidualBlock` implements a residual connection with two convolution layers, while `CNNFeatureSequence` builds a CNN feature extractor using multiple stages of residual blocks, allowing for sequence modeling from input spectrogram data.**",
   "id": "da6d99aa26dcf157"
  },
  {
   "cell_type": "code",
   "id": "a4114f54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T22:20:07.908289Z",
     "start_time": "2025-04-24T22:20:07.897554Z"
    }
   },
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dilation=1, downsample=None, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Constructs a Residual Block using two 2D convolutions with optional dilation and skip connections.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            dilation (int, optional): Dilation factor for convolutions, controls receptive field. Default is 1.\n",
    "            downsample (callable, optional): Optional 1x1 conv layer to match input/output channels when needed.\n",
    "            dropout (float, optional): Dropout rate applied between convolutions. Default is 0.0 (no dropout).\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        # First convolution layer with dilation\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               padding=dilation, dilation=dilation, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)  # Batch normalization for the first convolution\n",
    "        self.relu = nn.ReLU(inplace=True)  # Activation function\n",
    "        self.dropout = nn.Dropout2d(p=dropout) if dropout > 0 else nn.Identity()  # Dropout for regularization\n",
    "        # Second convolution layer with dilation\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               padding=dilation, dilation=dilation, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)  # Batch normalization for the second convolution\n",
    "        self.downsample = downsample  # Optional layer to match input and output dimensions for skip connections\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the Residual Block.\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [B, C, F, T].\n",
    "        Returns:\n",
    "            Tensor: Output tensor of the same shape as input (unless downsample changes channels).\n",
    "        \"\"\"\n",
    "        identity = x  # Save the original input for the skip connection\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)  # Match input dimensions with output dimensions using the downsample layer\n",
    "        out = self.conv1(x)  # First convolution\n",
    "        out = self.bn1(out)  # Normalize output\n",
    "        out = self.relu(out)  # Apply activation\n",
    "        out = self.dropout(out)  # Apply dropout if any\n",
    "        out = self.conv2(out)  # Second convolution\n",
    "        out = self.bn2(out)  # Normalize again\n",
    "        out += identity  # Add skip connection (residual)\n",
    "        out = self.relu(out)  # Activation after summing\n",
    "        return out\n",
    "\n",
    "class CNNFeatureSequence(nn.Module):\n",
    "    def __init__(self, input_channels=11, feature_dim=512, depths=(6, 6, 6), dropout=0.1):\n",
    "        \"\"\"\n",
    "        Constructs a CNN-based feature extractor for sequence modeling.\n",
    "        Args:\n",
    "            input_channels (int, optional): Number of input channels. Default is 11.\n",
    "            feature_dim (int, optional): Dimensionality of feature outputs. Default is 512.\n",
    "            depths (tuple of int): Number of residual blocks per stage. Default is (6, 6, 6).\n",
    "            dropout (float): Dropout rate applied in residual blocks. Default is 0.1.\n",
    "        \"\"\"\n",
    "        super(CNNFeatureSequence, self).__init__()\n",
    "        # Initial convolutional layer to process the input\n",
    "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=1, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)  # Batch normalization for the initial layer\n",
    "        self.relu = nn.ReLU(inplace=True)  # Activation function\n",
    "        # Pooling layer to downsample the frequency dimension\n",
    "        self.pool_freq = nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))\n",
    "\n",
    "        # Depths of each stage\n",
    "        d1, d2, d3 = depths\n",
    "        # Sequential container for all stages of residual blocks\n",
    "        self.layers = nn.Sequential(\n",
    "            *self._make_layer(64, 128, num_blocks=d1, dilation=1, dropout=dropout),\n",
    "            *self._make_layer(128, 256, num_blocks=d2, dilation=2, dropout=dropout),\n",
    "            *self._make_layer(256, 512, num_blocks=d3, dilation=4, dropout=dropout),\n",
    "        )\n",
    "\n",
    "        # Final projection layer to produce the expected feature dimension\n",
    "        self.projection = nn.Conv2d(512, feature_dim, kernel_size=1)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, dilation, dropout):\n",
    "        \"\"\"\n",
    "        Creates a sequence of residual blocks with optional dilation.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            num_blocks (int): Number of residual blocks.\n",
    "            dilation (int): Dilation factor for convolution layers.\n",
    "            dropout (float): Dropout rate for blocks.\n",
    "        Returns:\n",
    "            list[nn.Module]: A list of residual blocks to be used in a sequential layer.\n",
    "        \"\"\"\n",
    "        # Downsample layer to match input and output dimensions when necessary\n",
    "        downsample = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        ) if in_channels != out_channels else None\n",
    "\n",
    "        # Create the first residual block with downsampling\n",
    "        layers = [ResidualBlock(in_channels, out_channels, dilation=dilation, downsample=downsample, dropout=dropout)]\n",
    "        # Add remaining residual blocks without downsampling\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels, dilation=dilation, dropout=dropout))\n",
    "        return layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the CNN feature extractor.\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape [B, C, F, T], where\n",
    "                B = batch size,\n",
    "                C = input channels (e.g. spectrogram types),\n",
    "                F = frequency bins,\n",
    "                T = time frames.\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape [B, T, D * F′], where D is the projected feature size\n",
    "                    and F′ is the downsampled frequency dimension.\n",
    "                    This format is suitable for Transformer input as a time-major sequence.\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)  # Initial convolution\n",
    "        x = self.bn1(x)  # Batch normalization\n",
    "        x = self.relu(x)  # Activation function\n",
    "        x = self.pool_freq(x)  # Downsample frequency dimension\n",
    "        x = self.layers(x)  # Pass through residual blocks\n",
    "        x = self.projection(x)  # Project to feature_dim\n",
    "        x = x.permute(0, 3, 1, 2)  # Rearrange dimensions for sequence modeling (time-major format)\n",
    "        x = x.flatten(2)  # Flatten frequency dimension\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T22:20:17.948614Z",
     "start_time": "2025-04-24T22:20:07.918185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_split_with_spec_len_df = pd.read_pickle(\"train_split_with_spec_len.pkl\")\n",
    "train_split_with_spec_len_df = train_split_with_spec_len_df.dropna(subset=[\"Spectrogram_Path\"])\n",
    "\n",
    "# Create output directory for CNN features\n",
    "cnn_output_dir = Path(\"cnn_outputs\")\n",
    "os.makedirs(cnn_output_dir, exist_ok=True)\n"
   ],
   "id": "a2152297ad2453db",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T22:20:18.018309Z",
     "start_time": "2025-04-24T22:20:18.013261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_T = 2048  # Maximum number of time steps per chunk\n",
    "CHUNK_OVERLAP = 256  # Number of overlapping frames between chunks\n",
    "\n",
    "def chunk_tensor(tensor, max_t, overlap=0, pad_short=True):\n",
    "    \"\"\"\n",
    "    Splits an input tensor along the time dimension into chunks.\n",
    "    Args:\n",
    "        tensor (Tensor): Input tensor of shape [1, C, F, T].\n",
    "        max_t (int): Maximum allowed time frames per chunk.\n",
    "        overlap (int): Number of overlapping frames between consecutive chunks.\n",
    "        pad_short (bool): Whether to pad the final chunk if it's shorter than max_t.\n",
    "    Returns:\n",
    "        list[Tensor]: List of [1, C, F, T_chunk] tensors.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    stride = max_t - overlap\n",
    "    _, _, _, total_t = tensor.shape\n",
    "    for start in range(0, total_t, stride):\n",
    "        end = min(start + max_t, total_t)\n",
    "        chunk = tensor[..., start:end]\n",
    "        if chunk.shape[-1] < max_t:\n",
    "            if pad_short:\n",
    "                pad = max_t - chunk.shape[-1]\n",
    "                chunk = F.pad(chunk, (0, pad))  # Pad at the end\n",
    "            else:\n",
    "                break  # Skip if we don't want short tails\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ],
   "id": "449928ef81267038",
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "6adcba63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T22:20:18.082231Z",
     "start_time": "2025-04-24T22:20:18.076815Z"
    }
   },
   "source": [
    "class MultiHeadOutput(nn.Module):\n",
    "    \"\"\"\n",
    "    Predicts pitch, velocity_bin, duration_bin, and time_bin from transformer outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, pitch_classes=128, velocity_bins=32, duration_bins=64, time_bins=64):\n",
    "        super().__init__()\n",
    "        self.pitch_head = nn.Linear(d_model, pitch_classes)\n",
    "        self.velocity_head = nn.Linear(d_model, velocity_bins)\n",
    "        self.duration_head = nn.Linear(d_model, duration_bins)\n",
    "        self.time_head = nn.Linear(d_model, time_bins)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, D]\n",
    "        return {\n",
    "            \"pitch\": self.pitch_head(x),  # [B, T, pitch_classes]\n",
    "            \"velocity\": self.velocity_head(x),  # [B, T, velocity_bins]\n",
    "            \"duration\": self.duration_head(x),  # [B, T, duration_bins]\n",
    "            \"time\": self.time_head(x),  # [B, T, time_bins]\n",
    "        }\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "b9a68dea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T22:20:18.147126Z",
     "start_time": "2025-04-24T22:20:18.140430Z"
    }
   },
   "source": [
    "class WAVtoMIDIModel(nn.Module):\n",
    "    def __init__(self, input_channels=11, d_model=512,\n",
    "                 pitch_classes=128, velocity_bins=32, duration_bins=64, time_bins=64):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.cnn = CNNFeatureSequence(input_channels=input_channels, feature_dim=d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=8, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "        self.output_heads = MultiHeadOutput(d_model, pitch_classes, velocity_bins, duration_bins, time_bins)\n",
    "\n",
    "        # Not a module, will be set up later\n",
    "        self._dynamic_projection = None\n",
    "\n",
    "    def _ensure_projection(self, in_dim):\n",
    "        if self._dynamic_projection is None or self._dynamic_projection.in_features != in_dim:\n",
    "            self._dynamic_projection = nn.Linear(in_dim, self.d_model).to(next(self.parameters()).device)\n",
    "            # Register properly for training & saving\n",
    "            self.add_module(\"post_cnn_projection\", self._dynamic_projection)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 4:\n",
    "            # Input is spectrogram: [B, C=11, F, T]\n",
    "            x = self.cnn(x)  # ➜ [B, T, feature_dim]\n",
    "        elif x.dim() == 3:\n",
    "            # Input is precomputed features: [B, T, feature_dim]\n",
    "            pass  # Already suitable for projection\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported input shape: {x.shape}\")\n",
    "\n",
    "        self._ensure_projection(x.shape[-1])\n",
    "        x = self._dynamic_projection(x)\n",
    "        assert x.shape[-1] == self.d_model, f\"❌ Expected projected dim {self.d_model}, got {x.shape[-1]}\"\n",
    "        x = self.transformer(x)\n",
    "        return self.output_heads(x)\n",
    "\n",
    "    def transformer_only(self, features_tensor):\n",
    "        self._ensure_projection(features_tensor.shape[-1])\n",
    "        x = self._dynamic_projection(features_tensor)\n",
    "        return self.transformer(x)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T22:20:18.210019Z",
     "start_time": "2025-04-24T22:20:18.202986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_loss_classical(predictions, targets, component_weights=None, class_weights=None):\n",
    "    \"\"\"\n",
    "    Compute the total weighted loss for quantized classical music tokens.\n",
    "    Parameters:\n",
    "        predictions (dict): Model outputs with keys [\"pitch\", \"velocity\", \"duration\", \"time\"],\n",
    "            where each tensor has shape [B, T, C].\n",
    "        targets (dict): True labels with the same keys as predictions, each with shape [B, T].\n",
    "        component_weights (dict, optional): Scalar importance weights for each component.\n",
    "            Defaults to {\"pitch\": 1.0, \"velocity\": 0.3, \"duration\": 1.0, \"time\": 1.2}.\n",
    "        class_weights (dict, optional): Dictionary of torch.FloatTensor for per-class weights.\n",
    "            Keys correspond to components.\n",
    "    Returns:\n",
    "        Tuple[Tensor, dict]: Total loss as a tensor and individual component losses as a dictionary.\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    component_losses = {}\n",
    "\n",
    "    if component_weights is None:\n",
    "        component_weights = {\"pitch\": 1.0, \"velocity\": 0.3, \"duration\": 1.0, \"time\": 1.2}\n",
    "\n",
    "    for key in predictions:\n",
    "        pred = predictions[key].reshape(-1, predictions[key].shape[-1])  # [B*T, C]\n",
    "        target = targets[key].reshape(-1)  # [B*T]\n",
    "        C = pred.shape[-1]\n",
    "\n",
    "        # Check for invalid class indices BEFORE calling cross_entropy\n",
    "        if target.max() >= C or target.min() < 0:\n",
    "            print(f\"❌ Invalid class index in {key}: min={target.min().item()}, max={target.max().item()}, num_classes={C}\")\n",
    "            target = torch.clamp(target, 0, C - 1)  # Clamp to valid range to avoid crashing\n",
    "\n",
    "        # Compute loss\n",
    "        if class_weights and key in class_weights:\n",
    "            weight_tensor = class_weights[key].to(pred.device)\n",
    "            loss = F.cross_entropy(pred, target, weight=weight_tensor)\n",
    "        else:\n",
    "            loss = F.cross_entropy(pred, target)\n",
    "\n",
    "        weighted_loss = component_weights.get(key, 1.0) * loss\n",
    "        component_losses[key] = weighted_loss.item()\n",
    "        total_loss += weighted_loss\n",
    "\n",
    "    return total_loss, component_losses"
   ],
   "id": "dba2fdc72dfbbb7c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T22:20:18.270901Z",
     "start_time": "2025-04-24T22:20:18.266769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def timing_similarity(pred_time, target_time, p=2.0, tau=5.0):\n",
    "    \"\"\"\n",
    "    Computes a soft similarity score between predicted and target timing using a p-norm kernel.\n",
    "    \"\"\"\n",
    "    error = torch.abs(pred_time - target_time).float()\n",
    "    scaled_error = (error / tau).pow(p)\n",
    "    return torch.exp(-scaled_error)\n"
   ],
   "id": "e7fae0d653bc72ce",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T22:20:18.333812Z",
     "start_time": "2025-04-24T22:20:18.328151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_streak_compensated_score(preds, targets, alpha=1.0, beta=1.0):\n",
    "    \"\"\"\n",
    "    Calculates a score where longer streaks of correct tokens reduce the penalty from mistakes.\n",
    "\n",
    "    Args:\n",
    "        preds (dict): logits for each component, shape [B, T, C]\n",
    "        targets (dict): true class indices, shape [B, T]\n",
    "        alpha (float): reward weight for streak length\n",
    "        beta (float): penalty weight for mistakes\n",
    "\n",
    "    Returns:\n",
    "        float: streak-compensated performance score (higher is better)\n",
    "    \"\"\"\n",
    "    pred_pitch = torch.argmax(preds[\"pitch\"], dim=-1)\n",
    "    pred_velocity = torch.argmax(preds[\"velocity\"], dim=-1)\n",
    "    pred_duration = torch.argmax(preds[\"duration\"], dim=-1)\n",
    "    pred_time = torch.argmax(preds[\"time\"], dim=-1)\n",
    "\n",
    "    all_correct = (\n",
    "        (pred_pitch == targets[\"pitch\"]) &\n",
    "        (pred_velocity == targets[\"velocity\"]) &\n",
    "        (pred_duration == targets[\"duration\"]) &\n",
    "        (pred_time == targets[\"time\"])\n",
    "    ).int()  # [B, T]\n",
    "\n",
    "    scores = []\n",
    "    for sequence in all_correct:\n",
    "        streak = 0\n",
    "        streak_sum = 0\n",
    "        error_count = 0\n",
    "        for token in sequence:\n",
    "            if token:\n",
    "                streak += 1\n",
    "                streak_sum += streak  # reward increases with streak length\n",
    "            else:\n",
    "                error_count += 1\n",
    "                streak = 0\n",
    "        N = len(sequence)\n",
    "        reward = alpha * (streak_sum / N)\n",
    "        penalty = beta * (error_count / N)\n",
    "        scores.append(reward - penalty)\n",
    "\n",
    "    return sum(scores) / len(scores)"
   ],
   "id": "41e5b1ae8c4e9783",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T22:20:18.396120Z",
     "start_time": "2025-04-24T22:20:18.390649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_soft_streak_score(preds, targets, alpha=1.0, beta=1.0, p=2.0, tau=5.0):\n",
    "    pred_pitch = torch.argmax(preds[\"pitch\"], dim=-1)\n",
    "    pred_velocity = torch.argmax(preds[\"velocity\"], dim=-1)\n",
    "    pred_duration = torch.argmax(preds[\"duration\"], dim=-1)\n",
    "    pred_time = torch.argmax(preds[\"time\"], dim=-1)\n",
    "\n",
    "    pitch_correct = (pred_pitch == targets[\"pitch\"]).float()\n",
    "    velocity_correct = (pred_velocity == targets[\"velocity\"]).float()\n",
    "    duration_correct = (pred_duration == targets[\"duration\"]).float()\n",
    "    time_score = timing_similarity(pred_time, targets[\"time\"], p=p, tau=tau)\n",
    "\n",
    "    token_scores = pitch_correct * velocity_correct * duration_correct * time_score\n",
    "\n",
    "    scores = []\n",
    "    for sequence in token_scores:\n",
    "        streak = 0.0\n",
    "        streak_score = 0.0\n",
    "        for token_score in sequence:\n",
    "            if token_score > 0.5:  # Optional softness threshold\n",
    "                streak += 1\n",
    "                streak_score += streak\n",
    "            else:\n",
    "                streak = 0\n",
    "        scores.append(streak_score / len(sequence))\n",
    "\n",
    "    return alpha * torch.tensor(scores).mean().item() - beta * (1 - torch.tensor(scores).mean().item())\n"
   ],
   "id": "ee1564d51bd75a9f",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T22:20:18.461190Z",
     "start_time": "2025-04-24T22:20:18.454668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def collate_fn(batch):\n",
    "    # Filter out bad samples and keep track of original indices\n",
    "    batch = [(i, b) for i, b in enumerate(batch) if b is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None, None\n",
    "\n",
    "    indices, valid_batch = zip(*batch)\n",
    "    spectrograms, midi_tokens = zip(*valid_batch)\n",
    "\n",
    "    # Filter out spectrograms with incorrect channel count\n",
    "    filtered = []\n",
    "    for i, (spec, tokens) in enumerate(zip(spectrograms, midi_tokens)):\n",
    "        if spec.shape[0] != 11:\n",
    "            print(f\"❌ Spectrogram at idx {indices[i]} has shape {spec.shape}, expected 11 channels\")\n",
    "            continue\n",
    "        filtered.append((spec, tokens))\n",
    "\n",
    "    if len(filtered) == 0:\n",
    "        print(\"⚠️ All spectrograms had invalid channel counts\")\n",
    "        return None, None\n",
    "\n",
    "    spectrograms, midi_tokens = zip(*filtered)\n",
    "\n",
    "    # Stack spectrograms and pad MIDI tokens\n",
    "    midi_tokens = [torch.tensor(seq, dtype=torch.long) for seq in midi_tokens]\n",
    "    midi_tokens_padded = pad_sequence(midi_tokens, batch_first=True, padding_value=0)\n",
    "\n",
    "    try:\n",
    "        spectrograms = torch.stack(spectrograms)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error stacking spectrograms: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    print(\"\\n📦 Batch sizes:\")\n",
    "    for i, tok in enumerate(midi_tokens):\n",
    "        print(f\"  ▸ Token {i}: shape={tok.shape}\")\n",
    "    print(f\"Spectrogram shapes: {[s.shape for s in spectrograms]}\\n\")\n",
    "\n",
    "    return spectrograms, midi_tokens_padded"
   ],
   "id": "87d8b7c216e4caaa",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T22:20:18.540958Z",
     "start_time": "2025-04-24T22:20:18.529441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Quantizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        velocity_bins=32,\n",
    "        duration_bins=64,\n",
    "        duration_range=(0.01, 10.0),\n",
    "        time_bins=32,\n",
    "        time_range=(0.01, 8.0),\n",
    "    ):\n",
    "        self.velocity_bins = velocity_bins\n",
    "        self.duration_bins = duration_bins\n",
    "        self.time_bins = time_bins\n",
    "\n",
    "        # Velocity edges are implicit (fixed-width)\n",
    "        self.velocity_bin_size = 128 / velocity_bins\n",
    "\n",
    "        # Log-space bins for duration and time\n",
    "        self.duration_edges = np.logspace(np.log10(duration_range[0]), np.log10(duration_range[1]), num=duration_bins + 1)\n",
    "        self.time_edges = np.logspace(np.log10(time_range[0]), np.log10(time_range[1]), num=time_bins + 1)\n",
    "\n",
    "    def quantize_velocity(self, velocity):\n",
    "        \"\"\"Quantize velocity [0–127] into bins.\"\"\"\n",
    "        bin_idx = int(velocity // self.velocity_bin_size)\n",
    "        return min(bin_idx, self.velocity_bins - 1)\n",
    "\n",
    "    def quantize_duration(self, duration_sec):\n",
    "        \"\"\"Quantize duration (in seconds) into log-spaced bins.\"\"\"\n",
    "        bin_idx = np.digitize(duration_sec, self.duration_edges) - 1\n",
    "        return int(np.clip(bin_idx, 0, self.duration_bins - 1))\n",
    "\n",
    "    def quantize_time(self, time_sec):\n",
    "        \"\"\"Quantize time since the last note (in seconds) into log-spaced bins.\"\"\"\n",
    "        bin_idx = np.digitize(time_sec, self.time_edges) - 1\n",
    "        return int(np.clip(bin_idx, 0, self.time_bins - 1))\n",
    "\n",
    "    def inverse_velocity(self, bin_idx):\n",
    "        \"\"\"Approximate original velocity from bin.\"\"\"\n",
    "        return int((bin_idx + 0.5) * self.velocity_bin_size)\n",
    "\n",
    "    def inverse_duration(self, bin_idx):\n",
    "        \"\"\"Approximate original duration (seconds) from bin.\"\"\"\n",
    "        bin_idx = np.clip(bin_idx, 0, self.duration_bins - 1)\n",
    "        return float((self.duration_edges[bin_idx] + self.duration_edges[bin_idx + 1]) / 2)\n",
    "\n",
    "    def inverse_time(self, bin_idx):\n",
    "        \"\"\"Approximate original time gap (seconds) from bin.\"\"\"\n",
    "        bin_idx = np.clip(bin_idx, 0, self.time_bins - 1)\n",
    "        return float((self.time_edges[bin_idx] + self.time_edges[bin_idx + 1]) / 2)"
   ],
   "id": "e20949676b1299e5",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T22:20:18.619994Z",
     "start_time": "2025-04-24T22:20:18.616099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "quantizer = Quantizer(\n",
    "    velocity_bins=32,\n",
    "    duration_bins=64,\n",
    "    time_bins=64,\n",
    "    duration_range=(0.01, 5.0),   # reasonable bounds from distribution\n",
    "    time_range=(0.01, 5.0),       # ditto for time since last note\n",
    ")\n"
   ],
   "id": "423d31b6fd6f1ad9",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**This code defines a `MaestroDataset` class, a PyTorch `Dataset` for loading and preprocessing data for MIDI and spectrogram files.**\n",
    "\n",
    "- It drops rows with missing `Spectrogram_Path` or `Encoded_MIDI_Tokens`.\n",
    "- In `__getitem__`, it:\n",
    "  - Loads and validates the spectrogram file.\n",
    "  - Fixes spectrogram shape issues if needed.\n",
    "  - Validates the structure of `Encoded_MIDI_Tokens`.\n",
    "  - Returns processed `spectrogram` and `midi_tokens` as tensors.\n",
    "\n",
    "Invalid data is logged and skipped."
   ],
   "id": "3118c3cffd23f63d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T22:20:18.685767Z",
     "start_time": "2025-04-24T22:20:18.677900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MaestroDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        # Drop rows with missing spectrogram paths or encoded MIDI tokens\n",
    "        self.df = df.dropna(subset=[\"Spectrogram_Path\", \"Encoded_MIDI_Tokens\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        spectrogram_path = row[\"Spectrogram_Path\"]\n",
    "\n",
    "        if not spectrogram_path or not os.path.exists(spectrogram_path):\n",
    "            raise IndexError(f\"Missing spectrogram file at index {idx}: {spectrogram_path}\")\n",
    "\n",
    "        try:\n",
    "            with open(spectrogram_path, \"rb\") as f:\n",
    "                spectrogram = pickle.load(f)\n",
    "            if not isinstance(spectrogram, torch.Tensor):\n",
    "                spectrogram = torch.tensor(spectrogram, dtype=torch.float32)\n",
    "\n",
    "            # Ensure spectrogram shape is [11, F, T]\n",
    "            if spectrogram.ndim == 3 and spectrogram.shape[0] == 11:\n",
    "                pass\n",
    "            elif spectrogram.ndim == 2 and spectrogram.shape[0] == 11:\n",
    "                spectrogram = spectrogram.unsqueeze(0)\n",
    "            elif spectrogram.ndim == 3 and spectrogram.shape[1] == 11:\n",
    "                spectrogram = spectrogram.permute(1, 0, 2)\n",
    "            else:\n",
    "                raise IndexError(f\"Invalid spectrogram shape at index {idx}: {spectrogram.shape}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            raise IndexError(f\"Error loading spectrogram at index {idx}: {e}\")\n",
    "\n",
    "        # Load and validate MIDI tokens\n",
    "        midi_tokens = row[\"Encoded_MIDI_Tokens\"]\n",
    "        if not isinstance(midi_tokens, list) or len(midi_tokens) == 0:\n",
    "            raise IndexError(f\"Empty or invalid MIDI token list at index {idx}\")\n",
    "\n",
    "        if not all(isinstance(t, (list, tuple)) and len(t) == 4 for t in midi_tokens):\n",
    "            raise IndexError(f\"Malformed MIDI token structure at index {idx}: {midi_tokens[:3]}\")\n",
    "\n",
    "        # Quantize tokens\n",
    "        quantized_tokens = []\n",
    "        for pitch, velocity, duration, time in midi_tokens:\n",
    "            velocity_bin = quantizer.quantize_velocity(velocity)\n",
    "            duration_bin = quantizer.quantize_duration(duration)\n",
    "            time_bin = quantizer.quantize_time(time)\n",
    "            quantized_tokens.append((pitch, velocity_bin, duration_bin, time_bin))\n",
    "\n",
    "        midi_tokens = torch.tensor(quantized_tokens, dtype=torch.long)\n",
    "\n",
    "        if midi_tokens.ndim != 2 or midi_tokens.shape[1] != 4:\n",
    "            raise IndexError(f\"Unexpected MIDI tensor shape at index {idx}: {midi_tokens.shape}\")\n",
    "\n",
    "        return spectrogram, midi_tokens"
   ],
   "id": "eb49bbf95a98d90c",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T22:20:18.749343Z",
     "start_time": "2025-04-24T22:20:18.743651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = MaestroDataset(train_split_with_spec_len_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)"
   ],
   "id": "cb6c759b66b51925",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**This cell defines the `objective` function for an Optuna study to optimize hyperparameters for a model converting WAV to MIDI.**\n",
    "\n",
    "Key steps:\n",
    "1. **Clean Resources**: Frees GPU/CPU memory before training.\n",
    "2. **Hyperparameter Sampling**: Samples CNN depths, dropout, transformer layers, number of heads, and learning rate.\n",
    "3. **Model Setup**: Configures the model, transformer encoder, and optimizer with sampled hyperparameters.\n",
    "4. **Training Loop**: Processes batches - applies chunking if spectrograms are too long, computes predictions, loss, and performs backpropagation.\n",
    "5. **Streak Evaluation**: Computes soft streak scores for performance feedback.\n",
    "6. **Plateau Handling**: Adjusts penalty (`p`) if performance improvement stalls beyond a patience threshold.\n",
    "7. **Result Output**: Returns the negative mean score for minimization by Optuna."
   ],
   "id": "43922d92b1d5cac6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T22:20:18.853698Z",
     "start_time": "2025-04-24T22:20:18.807739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plateau_count = 0\n",
    "best_score = float(\"-inf\")\n",
    "p = 1.0  # Start soft\n",
    "delta_p = 0.5  # Penalty increment value\n",
    "max_p = 4.0  # Maximum penalty weight\n",
    "patience = 15  # Trials without significant improvement before increasing penalty\n",
    "\n",
    "# Store the best model per trial\n",
    "best_model_state = None\n",
    "best_trial_score = float(\"-inf\")\n",
    "\n",
    "use_fine_timing = False  # Start with snapped timing, refine later\n",
    "\n",
    "MAX_T = 1048  # Lowered max time length for chunking\n",
    "CHUNK_OVERLAP = 128\n",
    "MEMORY_THRESHOLD = 0.8  # 70% of combined memory (GPU + CPU fallback)\n",
    "\n",
    "import pynvml\n",
    "import psutil\n",
    "pynvml.nvmlInit()\n",
    "gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "def total_memory_bytes():\n",
    "    meminfo = pynvml.nvmlDeviceGetMemoryInfo(gpu_handle)\n",
    "    return meminfo.total + psutil.virtual_memory().total\n",
    "\n",
    "def used_memory_bytes():\n",
    "    meminfo = pynvml.nvmlDeviceGetMemoryInfo(gpu_handle)\n",
    "    return meminfo.used + psutil.virtual_memory().used\n",
    "\n",
    "def gpu_memory_fraction():\n",
    "    total = total_memory_bytes()\n",
    "    used = used_memory_bytes()\n",
    "    return used / total if total else 0\n",
    "\n",
    "def objective(trial):\n",
    "    global best_score, plateau_count, p, best_model_state, best_trial_score, use_fine_timing\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Sample hyperparameters\n",
    "    depths = (\n",
    "        trial.suggest_int(\"depth_stage1\", 2, 6),\n",
    "        trial.suggest_int(\"depth_stage2\", 2, 6),\n",
    "        trial.suggest_int(\"depth_stage3\", 2, 6),\n",
    "    )\n",
    "    dropout = 0.0  # 🔒 Force dropout off to save memory\n",
    "    num_layers = trial.suggest_int(\"num_transformer_layers\", 2, 8)\n",
    "    nhead = trial.suggest_categorical(\"nhead\", [4, 8, 16])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "\n",
    "    model = WAVtoMIDIModel(input_channels=11, d_model=512).to(device)\n",
    "    encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=nhead, batch_first=True)\n",
    "    model.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "\n",
    "    from torch.utils.checkpoint import checkpoint_sequential\n",
    "    model.cnn.layers.forward = lambda x: checkpoint_sequential(model.cnn.layers, segments=3, input=x, use_reentrant=False)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        if batch is None:\n",
    "            continue\n",
    "\n",
    "        spectrograms, midi_tokens = batch\n",
    "        t_lengths = [s.shape[-1] for s in spectrograms]\n",
    "        print(f\"\\U0001f9ea Spectrogram time lengths → min: {min(t_lengths)}, max: {max(t_lengths)}, avg: {sum(t_lengths) / len(t_lengths):.2f}\")\n",
    "        spectrograms = spectrograms.to(device)\n",
    "        midi_tokens = midi_tokens.to(device)\n",
    "\n",
    "        try:\n",
    "            targets = {\n",
    "                \"pitch\": midi_tokens[..., 0],\n",
    "                \"velocity\": midi_tokens[..., 1],\n",
    "                \"duration\": midi_tokens[..., 2],\n",
    "                \"time\": midi_tokens[..., 3],\n",
    "            }\n",
    "        except IndexError:\n",
    "            print(f\"⚠️ Bad token shape: {midi_tokens.shape}\")\n",
    "            return float(\"inf\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if spectrograms.shape[-1] > MAX_T:\n",
    "            chunks = chunk_tensor(spectrograms[0].unsqueeze(0), MAX_T, overlap=CHUNK_OVERLAP, pad_short=True)\n",
    "            all_preds = []\n",
    "            accumulated_tokens = []\n",
    "            for chunk in chunks:\n",
    "                current_fraction = gpu_memory_fraction()\n",
    "                if current_fraction >= MEMORY_THRESHOLD:\n",
    "                    print(f\"🚨 Combined memory usage exceeded {MEMORY_THRESHOLD*100:.0f}% ({current_fraction:.2f}), stopping chunk loading early.\")\n",
    "                    break\n",
    "                print(f\"🚦 Chunk shape: {chunk.shape}\")\n",
    "                chunk = chunk.to(device)\n",
    "                chunk = chunk.float()\n",
    "                feat = model.cnn(chunk)\n",
    "                if feat.dim() == 2:\n",
    "                    feat = feat.unsqueeze(0)\n",
    "                out = model.transformer_only(feat)\n",
    "                out = model.output_heads(out)\n",
    "                all_preds.append(out)\n",
    "                accumulated_tokens.append(MAX_T)\n",
    "                del chunk, feat, out\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            if not all_preds:\n",
    "                print(\"❌ No chunks processed before hitting memory limit.\")\n",
    "                # return float(\"inf\")\n",
    "                continue\n",
    "            final_preds = {}\n",
    "            for key in all_preds[0]:\n",
    "                final_preds[key] = torch.cat([p[key] for p in all_preds], dim=1)\n",
    "            preds = final_preds\n",
    "\n",
    "            # Truncate targets to match final prediction length\n",
    "            # chunked_length = sum(accumulated_tokens)\n",
    "            chunked_length = final_preds[\"pitch\"].shape[1]\n",
    "\n",
    "            for key in targets:\n",
    "                targets[key] = targets[key][:, :chunked_length]\n",
    "        else:\n",
    "            preds = model(spectrograms)\n",
    "\n",
    "        target_len = targets[\"pitch\"].shape[1]\n",
    "        for key in preds:\n",
    "            preds[key] = preds[key][:, :target_len, :]\n",
    "\n",
    "        if use_fine_timing:\n",
    "            print(\"🎯 Using fine timing targets\")\n",
    "\n",
    "        loss, _ = compute_loss_classical(preds, targets)\n",
    "        assert loss.requires_grad, \"Loss does not require grad — check no_grad() usage\"\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            score = compute_soft_streak_score(preds, targets, p=p)\n",
    "            scores.append(score)\n",
    "\n",
    "        if i >= 5:\n",
    "            break\n",
    "\n",
    "    mean_score = sum(scores) / len(scores)\n",
    "    improvement = mean_score - best_score\n",
    "\n",
    "    if improvement < 0.01:\n",
    "        plateau_count += 1\n",
    "    else:\n",
    "        best_score = mean_score\n",
    "        plateau_count = 0\n",
    "\n",
    "    if plateau_count >= patience and p < max_p:\n",
    "        p = min(max_p, p + delta_p)\n",
    "        print(f\"⏫ Increasing streak penalty: p → {p}\")\n",
    "        plateau_count = 0\n",
    "        use_fine_timing = True\n",
    "\n",
    "    if mean_score > best_trial_score:\n",
    "        best_trial_score = mean_score\n",
    "        best_model_state = model.state_dict()\n",
    "\n",
    "    return -mean_score"
   ],
   "id": "1944e69323bd158",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "study = ot.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100)"
   ],
   "id": "c5401165c5aa6f21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "storage_path = \"sqlite:///optuna_study_music_model.db\"\n",
    "study_name = \"music_model_tuning\"\n",
    "\n",
    "# Create a persistent study and migrate the trials\n",
    "persistent_study = ot.create_study(\n",
    "    direction=study.direction,\n",
    "    study_name=study_name,\n",
    "    storage=storage_path,\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "# Copy all trials from the in-memory study\n",
    "for trial in study.trials:\n",
    "    persistent_study.add_trial(trial)\n",
    "\n",
    "print(f\"✅ Study saved to: {storage_path}\")"
   ],
   "id": "66052fd4868b1f0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T22:20:20.605512Z",
     "start_time": "2025-04-24T22:20:18.909808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "study = ot.load_study(\n",
    "    study_name=\"music_model_tuning\",\n",
    "    storage=\"sqlite:///optuna_study_music_model.db\"\n",
    ")\n",
    "\n",
    "print(study.best_trial)"
   ],
   "id": "732ad398fb0a9f91",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenTrial(number=22, state=1, values=[0.9977099236566573], datetime_start=datetime.datetime(2025, 4, 20, 21, 36, 50, 758089), datetime_complete=datetime.datetime(2025, 4, 20, 22, 4, 20, 213042), params={'depth_stage1': 5, 'depth_stage2': 3, 'depth_stage3': 4, 'num_transformer_layers': 6, 'nhead': 4, 'lr': 0.0006127182157069003}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'depth_stage1': IntDistribution(high=6, log=False, low=2, step=1), 'depth_stage2': IntDistribution(high=6, log=False, low=2, step=1), 'depth_stage3': IntDistribution(high=6, log=False, low=2, step=1), 'num_transformer_layers': IntDistribution(high=8, log=False, low=2, step=1), 'nhead': CategoricalDistribution(choices=(4, 8, 16)), 'lr': FloatDistribution(high=0.001, log=True, low=1e-05, step=None)}, trial_id=23, value=None)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T22:20:20.677341Z",
     "start_time": "2025-04-24T22:20:20.668025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_params = study.best_params\n",
    "print(\"🔧 Best hyperparameters:\", best_params)"
   ],
   "id": "5221cf53379647b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Best hyperparameters: {'depth_stage1': 5, 'depth_stage2': 3, 'depth_stage3': 4, 'num_transformer_layers': 6, 'nhead': 4, 'lr': 0.0006127182157069003}\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T22:20:24.653714Z",
     "start_time": "2025-04-24T22:20:20.735945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Unpack the best parameters\n",
    "depths = (\n",
    "    best_params[\"depth_stage1\"],\n",
    "    best_params[\"depth_stage2\"],\n",
    "    best_params[\"depth_stage3\"]\n",
    ")\n",
    "num_layers = best_params[\"num_transformer_layers\"]\n",
    "nhead = best_params[\"nhead\"]\n",
    "lr = best_params[\"lr\"]\n",
    "\n",
    "# Create model\n",
    "model = WAVtoMIDIModel(input_channels=11, d_model=512).to(device)\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=nhead, batch_first=True)\n",
    "model.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ],
   "id": "a68704bc3ad2908e",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize the model with tuned parameters and move it to the correct device\n",
    "best_depths = depths\n",
    "best_dropout = 0.0       # From tuning\n",
    "input_channels = 11\n",
    "feature_dim = 512\n",
    "\n",
    "model = CNNFeatureSequence(\n",
    "    input_channels=input_channels,\n",
    "    feature_dim=feature_dim,\n",
    "    depths=best_depths,\n",
    "    dropout=best_dropout\n",
    ").to(device)\n",
    "model.eval()\n",
    "\n",
    "# Create a list to store paths to CNN feature outputs\n",
    "cnn_feature_paths = []\n",
    "\n",
    "# Process and save CNN outputs for each row in the DataFrame\n",
    "for idx, row in train_split_with_spec_len_df.iterrows():\n",
    "    feature_path = cnn_output_dir / f\"cnn_output_{idx}.pt\"\n",
    "\n",
    "    if feature_path.exists():\n",
    "        train_split_with_spec_len_df.at[idx, \"CNN_Feature_Path\"] = str(feature_path)\n",
    "        continue\n",
    "\n",
    "    print(f\"{idx}\")\n",
    "    try:\n",
    "        with open(row[\"Spectrogram_Path\"], \"rb\") as f:\n",
    "            spec = pickle.load(f)\n",
    "\n",
    "        if not isinstance(spec, torch.Tensor):\n",
    "            spec = torch.tensor(spec, dtype=torch.float32)\n",
    "\n",
    "        spec = spec.unsqueeze(0).to(device)  # Shape: [1, C, F, T]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if spec.shape[-1] > MAX_T:\n",
    "                chunks = chunk_tensor(spec, MAX_T, overlap=CHUNK_OVERLAP)\n",
    "                outputs = [model(chunk).squeeze(0).cpu() for chunk in chunks]\n",
    "                output = torch.cat(outputs, dim=0)\n",
    "            else:\n",
    "                output = model(spec).squeeze(0).cpu()\n",
    "\n",
    "        torch.save(output, feature_path)\n",
    "        train_split_with_spec_len_df.at[idx, \"CNN_Feature_Path\"] = str(feature_path)\n",
    "\n",
    "        del spec, output\n",
    "        gc.collect()\n",
    "        print(f\"RAM used: {psutil.virtual_memory().used / 1e9:.2f} GB\")\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            train_split_with_spec_len_df.to_pickle(\"train_split_with_cnn_paths_progress.pkl\")\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(f\"⚠️ RuntimeError at index {idx}: {e}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        train_split_with_spec_len_df.at[idx, \"CNN_Feature_Path\"] = None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Unexpected error at index {idx}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        train_split_with_spec_len_df.at[idx, \"CNN_Feature_Path\"] = None\n",
    "\n",
    "print(\"✅ Done.\")"
   ],
   "id": "52756a12cfd2acbe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_full_model(\n",
    "        model: torch.nn.Module,\n",
    "        train_loader: torch.utils.data.DataLoader,\n",
    "        epochs: int = 10,\n",
    "        use_fine_timing: bool = False,\n",
    "        checkpoint_dir: str = \"checkpoints\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Trains the specified model on the provided training data loader for a given number of epochs. This function\n",
    "    includes memory management for GPU usage and handles fine-grained timing sensitivity during training. It saves\n",
    "    the model's checkpoints at each epoch and retains the best model based on evaluation scores.\n",
    "\n",
    "    :param model: The model to be trained, equipped with methods for forward propagation and output prediction.\n",
    "    :param train_loader: An iterable data loader providing batches of training data.\n",
    "    :param epochs: The number of training epochs. Default is 10.\n",
    "    :param use_fine_timing: Boolean flag to enable or disable fine-grained timing sensitivity during training.\n",
    "    :type use_fine_timing: bool\n",
    "    :param checkpoint_dir: Directory path to store the model checkpoints.\n",
    "    :type checkpoint_dir: str\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    import traceback\n",
    "    import pynvml\n",
    "    import psutil\n",
    "\n",
    "    def gpu_memory_fraction():\n",
    "        try:\n",
    "            pynvml.nvmlInit()\n",
    "            handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "            gpu_mem = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "            cpu_mem = psutil.virtual_memory()\n",
    "            total = gpu_mem.total + cpu_mem.total\n",
    "            used = gpu_mem.used + cpu_mem.used\n",
    "            return used / total if total else 0\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not read GPU memory stats: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    model.train()\n",
    "    global p\n",
    "    p = 2.0  # Final fine-grained timing sensitivity\n",
    "\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    best_score = float(\"-inf\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        score_sum = 0.0\n",
    "        count = 0\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            try:\n",
    "                if batch is None:\n",
    "                    print(f\"⚠️ Skipping batch {batch_idx} due to None value\")\n",
    "                    continue\n",
    "\n",
    "                spectrograms, midi_tokens = batch\n",
    "                if spectrograms.shape[0] != midi_tokens.shape[0]:\n",
    "                    print(f\"❌ Mismatched batch size: spectrograms={spectrograms.shape[0]}, tokens={midi_tokens.shape[0]}\")\n",
    "                    continue\n",
    "\n",
    "                spectrograms = spectrograms.to(device)\n",
    "                midi_tokens = midi_tokens.to(device)\n",
    "\n",
    "                if midi_tokens.shape[-1] != 4:\n",
    "                    print(f\"❌ Unexpected MIDI token shape: {midi_tokens.shape}\")\n",
    "                    continue\n",
    "\n",
    "                targets = {\n",
    "                    \"pitch\": midi_tokens[..., 0],\n",
    "                    \"velocity\": midi_tokens[..., 1],\n",
    "                    \"duration\": midi_tokens[..., 2],\n",
    "                    \"time\": midi_tokens[..., 3],\n",
    "                }\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if spectrograms.shape[-1] > MAX_T:\n",
    "                    chunks = chunk_tensor(spectrograms[0].unsqueeze(0), MAX_T, overlap=CHUNK_OVERLAP, pad_short=True)\n",
    "                    all_preds = []\n",
    "                    for chunk_idx, chunk in enumerate(chunks):\n",
    "                        if gpu_memory_fraction() > MEMORY_THRESHOLD:\n",
    "                            print(f\"🚨 Memory threshold hit after chunk {chunk_idx}, skipping remaining chunks\")\n",
    "                            break\n",
    "                        chunk = chunk.to(device)\n",
    "                        feat = model.cnn(chunk)\n",
    "                        if feat.dim() == 2:\n",
    "                            feat = feat.unsqueeze(0)\n",
    "                        out = model.transformer_only(feat)\n",
    "                        preds = model.output_heads(out)\n",
    "                        all_preds.append(preds)\n",
    "                        del chunk, feat, out\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                    if not all_preds:\n",
    "                        print(f\"⚠️ No valid chunks processed for batch {batch_idx}\")\n",
    "                        continue\n",
    "\n",
    "                    final_preds = {k: torch.cat([p[k] for p in all_preds], dim=1) for k in all_preds[0]}\n",
    "                    preds = final_preds\n",
    "\n",
    "                    for key in targets:\n",
    "                        T_pred = preds[key].shape[1]\n",
    "                        T_target = targets[key].shape[1]\n",
    "                        min_T = min(T_pred, T_target)\n",
    "                        preds[key] = preds[key][:, :min_T, :]\n",
    "                        targets[key] = targets[key][:, :min_T]\n",
    "                else:\n",
    "                    preds = model(spectrograms)\n",
    "\n",
    "                loss, _ = compute_loss_classical(preds, targets)\n",
    "                if not loss.requires_grad:\n",
    "                    print(f\"❌ Loss does not require grad in batch {batch_idx}, skipping\")\n",
    "                    continue\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    score = compute_soft_streak_score(preds, targets, p=p)\n",
    "                    score_sum += score\n",
    "                    total_loss += loss.item()\n",
    "                    count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"🛑 Error in batch {batch_idx}: {e}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "        if count == 0:\n",
    "            print(\"⚠️ No valid batches processed in this epoch\")\n",
    "            continue\n",
    "\n",
    "        avg_loss = total_loss / count\n",
    "        avg_score = score_sum / count\n",
    "        print(f\"📚 Epoch {epoch+1}/{epochs} — Loss: {avg_loss:.4f}, Score: {avg_score:.4f}\")\n",
    "\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"epoch_{epoch+1:03d}_score_{avg_score:.4f}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "            'score': avg_score\n",
    "        }, checkpoint_path)\n",
    "\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            best_path = os.path.join(checkpoint_dir, \"best_model.pt\")\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print(f\"💾 Best model updated: {best_path}\")"
   ],
   "id": "f161c97e20778c89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_full_model(model, train_loader, epochs=20, use_fine_timing=True)",
   "id": "3912c32f5517892a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "89dad5536b9d1291",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Music_AI)",
   "language": "python",
   "name": "music_ai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
